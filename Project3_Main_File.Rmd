---
title: "Diabetes Data Analysis"
author: "Rocco Matarazzo & Karthik Edupuganti"
date: "`r Sys.Date()`"
output: 
  github_document:
      html_preview: false
params:
  eduLevel: Minimal Schooling
---

```{r libraries, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

# Calling basic libraries
library(readr)
library(tidyverse)
library(rmarkdown)
library(caret)
library(rsample)
library(naivebayes)
library(MASS)
library(ggcorrplot)

# Education levels from https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/discussion/295407
# 1 Never attended school or only kindergarten
# 2 Grades 1 through 8 (Elementary)
# 3 Grades 9 through 11 (Some high school)
# 4 Grade 12 or GED (High school graduate)
# 5 College 1 year to 3 years (Some college or technical school)
# 6 College 4 years or more (College graduate)
# 9 Refused (All these records were deleted from the dataset)
```




# Introduction

This report serves as an analysis on 2015 dataset with over 250,000 survey responses pertaining to diabetes. The main variable of interest is a binary indicator for diabetes, of course, where 1 represents diabetes/prediabetes and 0 represents no diabetes. 

Other variables that supplement our analysis include HighBP, HighChol, BMI, PhysActivity, Fruits, Veggies, Sex, Age, Smoker and Stroke. Each of these are binary variables, except for BMI and Age. BMI represents the individual's Body Mass Index, which can help identify potential health problems by using the person's height and weight. HighBP, HighChol, Smoker, and Stroke are each coded as 1 equals yes, and 0 equals no. For example, an individual that has a value of 1 in each of these categories has high blood pressure, high cholesterol, classifies as a smoker, and has had a stroke in their life. To classify as a smoker in this dataset, an individual must have smoked at least 100 cigarettes within their lifetime at the time of the survey. 

Just as the variables listed above, PhysActivity, Fruits, and Veggies are another set of yes (1) and no (0) variables. PhysActivity represents whether or not the individual exercised within the last month at the time of survey, other than their regular job. Fruits and veggies determine whether the individual has at least one fruit or vegetable per day. A value of 1 in each of these categories would imply a rather healthy lifestyle. Finally, Sex represents the gender of the individual -- 1 for male and 0 for female -- and Age has several splits, but ultimately ranges from 18 years old to 80 or older. If you are interested in more information about the dataset, please visit the following [link.](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/)

#### TO DO
*describes the purpose of your EDA and modeling, along with the end result you will be creating.*

# Data

Here we are reading in our data. First, we uploaded the data to our repo, set our repo as our working directory, then read the file using the read_csv() function. Also, to have a clearer understanding of the data we're working with, we used the dplyr package and mutate() function to recreate several of the categorical variables with string/character level names. 

We combined the first group (No school/kindergarten) and the second group (Elementary level) into one group called "Minimal Schooling." This way we have 5 distinct groups as follows:  
- Minimal Schooling  
- Some High School  
- High School Graduate  
- Some College  
- College Graduate

```{r, results=FALSE, message=FALSE, warning=FALSE}

# set Repo as your working directory first
diabetes <- as_tibble(read_csv("diabetes_binary_health_indicators_BRFSS2015 (1).csv"))

# Adding meaningful names to levels of variables
diabetes_full <- 
  diabetes %>% 
   mutate(
  # Education Level   
  Education = case_when(
    Education == 1 ~ "Minimal Schooling",
    Education == 2 ~ "Minimal Schooling",
    Education == 3 ~ "Some High School",
    Education == 4 ~ "High School Graduate",
    Education == 5 ~ "Some College",
    Education == 6 ~ "College Graduate"
  ),
  # Income
  Income = case_when(
    Income == 1 ~"< $10,000",
    Income == 2 ~"$10,000 <= Income < $15,000",
    Income == 3 ~"$15,000 <= Income < $20,000",
    Income == 4 ~"$20,000 <= Income < $25,000",
    Income == 5 ~"$25,000 <= Income < $35,000",
    Income == 6 ~"$35,000 <= Income < $50,000",
    Income == 7 ~"$50,000 <= Income < $75,000",
    Income == 8 ~"> $75,000"
  ),
    # Age
  Age = case_when(
    Age == 1  ~ "Age 18 to 24",    
    Age == 2  ~ "Age 25 to 29",
    Age == 3  ~ "Age 30 to 34",    
    Age == 4  ~ "Age 35 to 39",
    Age == 5  ~ "Age 40 to 44",    
    Age == 6  ~ "Age 45 to 49",
    Age == 7  ~ "Age 50 to 54",    
    Age == 8  ~ "Age 55 to 59",
    Age == 9  ~ "Age 60 to 64",    
    Age == 10 ~ "Age 65 to 69",
    Age == 11 ~ "Age 70 to 74",    
    Age == 12 ~ "Age 75 to 79",
    Age == 13 ~ "Age 80 or older"
  ),
  # High Blood Pressure
  HighBP = case_when(
    HighBP == 1 ~ "Yes",
    HighBP == 0 ~ "No"
  ), 
  # High Cholesterol
  HighChol = case_when(
    HighChol == 1 ~ "Yes",
    HighChol == 0 ~ "No"
  ), 
  # Physical Activity
  PhysActivity = case_when(
    PhysActivity == 1 ~ "Yes",
    PhysActivity == 0 ~ "No"
  ), 
  # Fruits
  Fruits = case_when(
    Fruits == 1 ~ "At least one piece of fruit per day",
    Fruits == 0 ~ "Diet contains no fruits"
  ), 
  # Vegetables
  Veggies = case_when(
    Veggies == 1 ~ "At least one vegetable per day",
    Veggies == 0 ~ "Diet contains no vegetables"
  ),
  # Sex
  Sex = case_when(
    Sex == 1 ~ "Male",
    Sex == 0 ~ "Female"
  ), 
  # Smoker
  Smoker = case_when(
    Smoker == 1 ~ "Yes",
    Smoker == 0 ~ "No"
  ),
  # Stroke
  Stroke = case_when(
    Stroke == 1 ~ "Yes",
    Stroke == 0 ~ "No"
  ),
  Diabetes_binary = case_when(
    Diabetes_binary == 1 ~ "Yes",
    Diabetes_binary == 0 ~ "No"
  )
)
# Subsetting the data down on param. of interest for particular analysis
markdown_data <- diabetes_full %>% filter(Education == params$eduLevel)

# Converting all variables to factors
markdown_data$Income <- factor(markdown_data$Income)
markdown_data$Age <- factor(markdown_data$Age)
markdown_data$HighBP <- factor(markdown_data$HighBP)
markdown_data$HighChol <- factor(markdown_data$HighChol)
markdown_data$PhysActivity <- factor(markdown_data$PhysActivity)
markdown_data$Fruits <- factor(markdown_data$Fruits)
markdown_data$Veggies <- factor(markdown_data$Veggies)
markdown_data$Sex <- factor(markdown_data$Sex)
markdown_data$Smoker <- factor(markdown_data$Smoker)
markdown_data$Stroke <- factor(markdown_data$Stroke)
markdown_data$Diabetes_binary <- factor(markdown_data$Diabetes_binary)

```

# Summarizations 
This section is where we will begin Exploratory Data Analysis (EDA). There are several techniques when conducting an EDA, including finding summary statistics and plotting various items from the dataset to potentially identify patterns or relationships within the data.

### Contingency Tables
First let's focus on HighBP, HighChol, Smoker, and Stroke. All four of these variables can imply poor health in comparison to not being associated with these variables.

```{r}
table(markdown_data$HighChol, markdown_data$HighBP, markdown_data$Diabetes_binary,  
      dnn = c("HighChol", "HighBP", "Diabetes"))


table(markdown_data$Stroke, markdown_data$Smoker, markdown_data$Diabetes_binary,  
      dnn = c("Stroke", "Smoker", "Diabetes"))
```

Next, we can explore the gender and age of the individual.
```{r}
table(markdown_data$Age, markdown_data$Diabetes_binary,  
      dnn = c("Age", "Diabetes"))


table(markdown_data$Sex, markdown_data$Diabetes_binary,  
      dnn = c("Sex", "Diabetes"))
```


### Plots
Next, let's investigate BMI, whether or not a person has diabetes, and Age on a plot. A higher BMI can indicate high body fat, which can lead to health problems such as diabetes. 

```{r}

ggplot(markdown_data, aes(x=BMI, y=Age, col=Diabetes_binary)) + 
  geom_point() +
  xlab("Body Mass Index (BMI)") +
  ylab("Age") +
  theme_bw() 

```

The next plot will display an individual's physical activity and dietary habits in relation to their BMI and whether or not they have diabetes. 

```{r}

ggplot(markdown_data, aes(x=BMI, y=PhysActivity, col=Diabetes_binary)) + 
  geom_point() +
  xlab("Body Mass Index (BMI)") +
  ylab("Participates In Physical Activity") +
  theme_bw() 

ggplot(markdown_data, aes(x=BMI, y=Fruits, col=Diabetes_binary)) + 
  geom_point() +
  xlab("Body Mass Index (BMI)") +
  ylab("Fruit Intake") +
  theme_bw() 


ggplot(markdown_data, aes(x=BMI, y=Veggies, col=Diabetes_binary)) + 
  geom_point() +
  xlab("Body Mass Index (BMI)") +
  ylab("Vegetable Intake") +
  theme_bw() 

```


# Modeling

In this section we will fit several models on our dataset with the predictor being the aforementioned binary diabetes variable. First, we'll split our data into a training and testing set. 

```{r}
# # set seed for reproducible purposes
# set.seed(1234)
# 
# # actually splitting the data
# # the 0.7 at the end implies 70% of data goes into the training set
# train <- sample(1:nrow(markdown_data), size = nrow(markdown_data)*0.70)
# # the test set will be the rows from markdown_data not in the training set
# test <- dplyr::setdiff(1:nrow(markdown_data), train)

# Set seed for reproducible purposes
set.seed(1234)


# p = 0.70 at the end implies 70% of data goes into the training set
trainingIndex <- createDataPartition(markdown_data$Diabetes_binary, p=0.70, list=FALSE)

# Actually splitting into testing and training
train <- markdown_data[trainingIndex,]
test <- markdown_data[-trainingIndex,]

```

### Log Loss

Log Loss is one of many metrics available to evaluate classification models. The lower the Log Loss, the better predictability a model has. A classification model will compute the probability that given a set of variables, how likely is it that this set belongs to the diabetes or no diabetes class. Therefore, the farther our predicted probability is from the observed class (0 or 1), the higher the Log Loss. 

We may prefer Log Loss to accuracy given it's more complex relation with the entire model. Accuracy simply determines how well our model classified data correctly and only cares about the observed class. Accuracy does not take into account the probability/uncertainty associated with the prediction like Log Loss does. 

### Logistic Regression
*would like to improve this section*
Logistic Regression is a very reasonable choice for our analysis. This is because of the nature of our target variable being binary. Logistic Regression has no linearity assumptions, which can be a benefit when we're dealing with a binary target variable like diabetes. We are able to model the average number of success for a given x, or rather the average number of "yes" answers for the given slate of variables in this instance. 

We fit three models here. The first includes all the variables of interest. The second model removes diet items such as the Fruits and Veggies variables, along with the Stroke variable. The third model includes interaction terms between Age and Sex, Smoker and Stroke, and BMI and PhysActivity.

```{r}
ctrl <- trainControl(method = "cv", classProbs = TRUE, number = 5, summaryFunction = mnLogLoss)

# Model 1, all variables
logistic_model_1 <- train(Diabetes_binary ~ HighBP + HighChol + BMI + PhysActivity + Fruits + Veggies + Sex + Age + Smoker + Stroke, data = train, method = "glm", family = "binomial", trControl = ctrl, metric = "logLoss")

# Model 2, removing Fruits, Veggies, and Stroke
logistic_model_2 <- train(Diabetes_binary ~ HighBP + HighChol + BMI + PhysActivity + Sex + Age + Smoker, data = train, method = "glm", family = "binomial", trControl = ctrl, metric = "logLoss")

# Model 3, adding interactions
logistic_model_3 <- train(Diabetes_binary ~ HighBP + HighChol + BMI*PhysActivity + Age*Sex + Smoker*Stroke, data = train, method = "glm", family = "binomial", trControl = ctrl, metric = "logLoss")

paste("The lowest log loss among these models is:",
      round(min(
logistic_model_1$results$logLoss,
logistic_model_2$results$logLoss,
logistic_model_3$results$logLoss), 4)
      )


if (min(
logistic_model_1$results$logLoss,
logistic_model_2$results$logLoss,
logistic_model_3$results$logLoss)
  ==
logistic_model_1$results$logLoss
) {
  
  logistic_best_model <- logistic_model_1
  print("The best model here is model 1.")
  
} else if(min(
logistic_model_1$results$logLoss,
logistic_model_2$results$logLoss,
logistic_model_3$results$logLoss)
  ==
logistic_model_2$results$logLoss
) {
  
  logistic_best_model <- logistic_model_2
  print("The best model here is model 2.")
  
} else {
  
  logistic_best_model <- logistic_model_3
  print("The best model here is model 3.")
  
}

```


### LASSO Regression
*section unfinished*
LASSO regression uses shrinkage methods in an attempt to improve predictions.
```{r}
lasso_model <- train(Diabetes_binary ~ HighBP + HighChol + BMI + PhysActivity + Fruits + Veggies + Sex + Age + Smoker + Stroke, data = train, method = "glmnet", tuneGrid = expand.grid(alpha = 1, lambda = seq(0, 1, by = 0.1)), trControl = ctrl, metric = "logLoss")

```

### Classification Tree

A classification tree is used when our goal is to classify group membership, in this case, classify whether or not an individual has diabetes. Like any tree based method, a classification tree will split the predictor space into many regions and have different predictions for each region. The tree is split by minimizing the Gini Coefficient, a number that represents how well each node classifies. 

```{r}
classification_tree_model <- train(Diabetes_binary ~ HighBP + HighChol + BMI + PhysActivity + Fruits + Veggies + Sex + Age + Smoker + Stroke, data = train, method = "rpart", tuneGrid = expand.grid(cp = seq(0,1, by = 0.001)), trControl = ctrl, metric = "logLoss")
```


### Random Forest

A Random Forest creates multiple bagged trees then averages the results. For each tree a random subset of predictors is chosen to fit the tree. This way, a single strong predictor won't dominate the tree fits as it will be left out of some of the other trees by random chance. We might choose the Random Forest over the Classification Tree because the Random Forest is typically stronger in prediction. Due to averaging over a large number of tree fits the variance decreases and prediction improves.

```{r}
random_forest_model <- train(Diabetes_binary ~ HighBP + HighChol + BMI + PhysActivity + Fruits + Veggies + Sex + Age + Smoker + Stroke, data = train, method = "rf", tuneGrid = expand.grid(mtry = 1:10), trControl = ctrl, metric = "logLoss")
```


### Non-Class Model #1

```{r}
naive_bayes_model <- train(Diabetes_binary ~ HighBP + HighChol + BMI + PhysActivity + Fruits + Veggies + Sex + Age + Smoker + Stroke, data = train, method = "naive_bayes", trControl = ctrl, metric = "logLoss")
```

### Non-Class Model #2

```{r}
lda_model <- train(Diabetes_binary ~ HighBP + HighChol + BMI + PhysActivity + Fruits + Veggies + Sex + Age + Smoker + Stroke, data = train, method = "lda", trControl = ctrl, metric = "logLoss")
```


# Final Model Selection
Here we will choose the best model based on RMSE.

```{r}

```


# Automation (Ignore for now)
This is for rendering all five docs (ignore for now)
```{r, eval=FALSE}
# # Setting Education Levels vector
# education_levels <- (c("Minimal Schooling", # dataset value = 1/2
#                       "Some High School", # dataset value = 3
#                       "High School Graduate", # dataset value = 4
#                       "Some College", # dataset value = 5
#                       "College Graduate")) # dataset value = 6
# 
# # adding the .html to each
# output_file <- paste0(education_levels, ".md")
# 
# params = lapply(education_levels, FUN = function(x){list(eduLevel = x)})
# 
# # finalizing and combining it in a tibble
# reports <- tibble(output_file, params)
# 
# # now we can apply! 
# apply(reports, MARGIN = 1,
#       FUN = function(x){
#         render(input = "Project3_Main_File.rmd",
#                output_file = x[[1]],
#                output_format = "github_document",
#                params = x[[2]])
#       })
```

